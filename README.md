trained on 1.6 million tweets dataset, use LDA, berts models

The pre-trained bert model and tokenizer files are too big to upload on GitHub. You've got to run the trainer files to train your own model.
